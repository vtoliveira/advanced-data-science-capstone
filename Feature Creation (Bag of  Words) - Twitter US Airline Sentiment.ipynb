{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Data Science Capstone - Week 2 - Feature Engineering/Creation and ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will work on process known as feature engineering. Basically, we will use information extracted from data understand notebook and specific knowledge of the problem (in this case, text mining) to format data in order to use a predictive model such as SVM or Naive Bayes. Therefore, we will also create features to help our classifier using techniques well explored in the text mining field. Later we will compare these approaches to the ones using Deep Learning and automatic feature extraction for this specific case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the strategies found on [1], we will pre-process in this way:\n",
    "   - Remove all URLs (e.g. www.xyz.com), hash tags (e.g. #topic), targets (@username)\n",
    "   - Correct the spellings; sequence of repeated characters is to be handled\n",
    "   - Replace all the emoticons with their sentiment.\n",
    "   - Remove all punctuations ,symbols, numbers\n",
    "   - Remove Stop Words\n",
    "   - Remove Non-English Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing data\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Tweets.csv\")\n",
    "\n",
    "# checking its dimensions\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# function to help us developing features such as lower_case, punctuation (keep or strip),\n",
    "# delete stop_words, etc.\n",
    "def preprocess(s, preserve_case=False, strip_handles=False, reduce_len=False, \n",
    "               punctuation = False, stop_words = False, join = True):\n",
    "    \n",
    "    punctuation = [] if punctuation else list(string.punctuation+'”“’')\n",
    "    stop = stopwords.words('english') + punctuation + ['rt', 'via'] if stop_words else punctuation + ['rt', 'via']\n",
    "    tknzr = TweetTokenizer(preserve_case=preserve_case, \n",
    "                           strip_handles=strip_handles, reduce_len=reduce_len)\n",
    "    \n",
    "    tokens = tknzr.tokenize(s)\n",
    "    \n",
    "    tokens = [token for token in tokens\n",
    "              if token not in stop and\n",
    "              not token.startswith(('#', '@','http', '...'))] \n",
    "    \n",
    "    if join:\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet: @JetBlue quick ? Why is a person traveling w  a mosaic not get the green tag? Doesn't make sense I end up waitin 4 my sons bag anyway :/\n",
      "Preprocessed : quick ? why is a person traveling w a mosaic not get the green tag ? doesn't make sense i end up waitin 4 my sons bag anyway :/\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "rnd = random.randint(0, data.shape[0])\n",
    "stop_words = False\n",
    "punctuation = False\n",
    "\n",
    "print(\"Original tweet: {}\".format(data.text.values[7804]))\n",
    "print(\"Preprocessed : {}\".format(preprocess(data.text.values[7804], join=True, punctuation=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Feature Extraction/Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing for Bag of Words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we have the following alternatives:\n",
    "   - Parts of Speech Tags\n",
    "   - Opinion words and phrases\n",
    "   - Position of terms\n",
    "   - Negation\n",
    "   - Presence of emoticons\n",
    "   - Stemming and lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will create different strategies based on feature engineering process. Overall, we will create different datasets to test our first models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['What said',\n",
       "       \"plus you've added commercials to the experience tacky\",\n",
       "       \"I didn't today Must mean I need to take another trip\"], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_preprocessed'] = data.text.apply(lambda x: preprocess(x, preserve_case=True, punctuation=False))\n",
    "data['text_preprocessed'].values[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use the library defined on nltk module called vader that contains a lexicon of positive, neutral and negative words. Here, we want to create features based on the polarity scores of our tweets, i.e., we will pass each tweet to the function polarity_scores, and it will returns values for negative, neutral, positive and compound. Then, they will be used as extra features to our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>text_preprocessed</th>\n",
       "      <th>neg_scores</th>\n",
       "      <th>neu_scores</th>\n",
       "      <th>pos_scores</th>\n",
       "      <th>compound_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355008</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:46 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>leaving over 20 minutes Late Flight No warning...</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.7906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398336</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:15 -0800</td>\n",
       "      <td>Nigeria,lagos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please bring American Airlines to</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634433</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:02 -0800</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>you have my money you change my flight and don...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866689</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:58:51 -0800</td>\n",
       "      <td>dallas, TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we have 8 ppl so we need 2 know how many seats...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.951</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.0772</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "14636  569587371693355008          negative                        1.0000   \n",
       "14637  569587242672398336           neutral                        1.0000   \n",
       "14638  569587188687634433          negative                        1.0000   \n",
       "14639  569587140490866689           neutral                        0.6771   \n",
       "\n",
       "               negativereason  negativereason_confidence   airline  \\\n",
       "14636  Customer Service Issue                     1.0000  American   \n",
       "14637                     NaN                        NaN  American   \n",
       "14638  Customer Service Issue                     0.6659  American   \n",
       "14639                     NaN                     0.0000  American   \n",
       "\n",
       "      airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "14636                    NaN    itsropes                 NaN              0   \n",
       "14637                    NaN    sanyabun                 NaN              0   \n",
       "14638                    NaN  SraJackson                 NaN              0   \n",
       "14639                    NaN   daviddtwu                 NaN              0   \n",
       "\n",
       "                                                    text tweet_coord  \\\n",
       "14636  @AmericanAir leaving over 20 minutes Late Flig...         NaN   \n",
       "14637  @AmericanAir Please bring American Airlines to...         NaN   \n",
       "14638  @AmericanAir you have my money, you change my ...         NaN   \n",
       "14639  @AmericanAir we have 8 ppl so we need 2 know h...         NaN   \n",
       "\n",
       "                   tweet_created tweet_location               user_timezone  \\\n",
       "14636  2015-02-22 11:59:46 -0800          Texas                         NaN   \n",
       "14637  2015-02-22 11:59:15 -0800  Nigeria,lagos                         NaN   \n",
       "14638  2015-02-22 11:59:02 -0800     New Jersey  Eastern Time (US & Canada)   \n",
       "14639  2015-02-22 11:58:51 -0800     dallas, TX                         NaN   \n",
       "\n",
       "                                       text_preprocessed  neg_scores  \\\n",
       "14636  leaving over 20 minutes Late Flight No warning...       0.296   \n",
       "14637                  Please bring American Airlines to       0.000   \n",
       "14638  you have my money you change my flight and don...       0.000   \n",
       "14639  we have 8 ppl so we need 2 know how many seats...       0.000   \n",
       "\n",
       "       neu_scores  pos_scores  compound_scores  \n",
       "14636       0.704       0.000          -0.7906  \n",
       "14637       0.635       0.365           0.3182  \n",
       "14638       0.885       0.115           0.3818  \n",
       "14639       0.951       0.049           0.0772  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def polarity_scores_all(data_text_column):\n",
    "    neg, neu, pos, compound = [], [], [], []\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "    for text in data_text_column:\n",
    "        dict_ = analyser.polarity_scores(text)\n",
    "        neg.append(dict_['neg'])\n",
    "        neu.append(dict_['neu'])\n",
    "        pos.append(dict_['pos'])\n",
    "        compound.append(dict_['compound'])\n",
    "    \n",
    "    return neg, neu, pos, compound\n",
    "\n",
    "all_scores = polarity_scores_all(data.text_preprocessed.values)\n",
    "data['neg_scores'] = all_scores[0]\n",
    "data['neu_scores'] = all_scores[1]\n",
    "data['pos_scores'] = all_scores[2]\n",
    "data['compound_scores'] = all_scores[3]\n",
    "data.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presence of emoticons and marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we process we replace each emoticon for its representation feeling such as SAD, HAPPY. Also, we will use INTERROGATION AND EXCLAMATION for indication of marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we replace emoticons based on its value: _SAD_, _HAPPY_, _NEUTRAL_\n",
    "# Also, we include markers just as _INTERROGATION_ and _EXCLAMATION_\n",
    "positive_emoticons = [':-)', ':)', '=)', ':D', ';D', ':]', ';]', ': D']\n",
    "negative_emoticons = [':-(', ':(', '=(', ';(', 'D:', 'D;', ':[', ';[', ':/']\n",
    "\n",
    "def emoticon_detection(raw_string):\n",
    "    list_emoticons = []\n",
    "    s = preprocess(raw_string, punctuation=True, join=False)\n",
    "    for token in s:\n",
    "        if token in positive_emoticons:\n",
    "            list_emoticons.append((token, 'HAPPY'))\n",
    "        if token in negative_emoticons:\n",
    "            list_emoticons.append((token, 'SAD'))\n",
    "        if token in ['?']:\n",
    "            list_emoticons.append((token, 'INTERROGATION'))\n",
    "        if token in ['!']:\n",
    "            list_emoticons.append((token, 'EXCLAMATION'))\n",
    "            \n",
    "    s = ' '.join(s)   \n",
    "                                  \n",
    "    for emoticon in list_emoticons:\n",
    "        s = s.replace(emoticon[0], emoticon[1])\n",
    "    return s                       \n",
    "\n",
    "data['text_preprocessed_with_emoticon'] = data.text.apply(lambda x: emoticon_detection(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is another common step in text analysis. Here, we will replace words for its root based on a rule called Porter Stemmer. From nlkt package, we have some examples: http://www.nltk.org/howto/stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5282\n",
      "Original tweet: @SouthwestAir Any way that I can get a receipt for a Cancelled Flightled portion of a roundtrip flight? Used the flight voucher just need receipt.\n",
      "Preprocessed : Any way that I can get a receipt for a Cancelled Flightled portion of a roundtrip flight Used the flight voucher just need receipt\n",
      "Preprocessed with stemming: way get receipt cancel flightl portion roundtrip flight use flight voucher need receipt\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def stemming(s):\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = preprocess(s, stop_words=True, join=False)\n",
    "    x = [stemmer.stem(w) for w in tokens]\n",
    "    \n",
    "    return ' '.join(x)\n",
    "\n",
    "rnd = random.randint(0, data.shape[0])\n",
    "stop_words = False\n",
    "\n",
    "print(rnd)\n",
    "print(\"Original tweet: {}\".format(data.text.values[rnd]))\n",
    "print(\"Preprocessed : {}\".format(preprocess(data.text.values[rnd], True)))\n",
    "print(\"Preprocessed with stemming: {}\".format(stemming(data.text.values[rnd])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming must be done after preprocessed text, because we do not want to remove interrogation marks,\n",
    "# exclamation marks, emoticons, etc.\n",
    "data['text_stemming_with_emoticon'] = data.text_preprocessed_with_emoticon.apply(lambda x: stemming(x))\n",
    "data['text_stemming'] = data.text_preprocessed.apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us air asham servic hold hour interrog help repres reschedul due weather interrog\n",
      "****************************************************************************************************\n",
      "us air asham servic hold hour help repres reschedul due weather\n"
     ]
    }
   ],
   "source": [
    "n = 10682\n",
    "\n",
    "print(data['text_stemming_with_emoticon'].values[n])\n",
    "print('*'*100)\n",
    "print(data['text_stemming'].values[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in [1], negation can be a feature that could be extremely informative about the opinion of a text. Here, we will create a variable indicating 1 if we have negation and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.73026\n",
       "True     0.26974\n",
       "Name: negation, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import negated\n",
    "\n",
    "data['negation'] = data.text_preprocessed.apply(lambda x: negated(x.split()))\n",
    "data['negation'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS - Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'today': 'NN',\n",
       " 'must': 'MD',\n",
       " 'mean': 'VB',\n",
       " 'need': 'MD',\n",
       " 'take': 'VB',\n",
       " 'anoth': 'DT',\n",
       " 'trip': 'NN'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "dict(nltk.pos_tag(data.text_stemming.values[2].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'NOUN' : ['NN','NNS','NNP','NNPS'],\n",
    "    'PRON' : ['PRP','PRP$','WP','WP$'],\n",
    "    'VERB' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'ADJ' :  ['JJ','JJR','JJS'],\n",
    "    'ADV' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "\n",
    "def count_pos_tag(data_text):\n",
    "    total_count = []\n",
    "    for s in data_text.text_preprocessed.values:\n",
    "        partial_count = {}\n",
    "        s = s.split()\n",
    "        count_pos = Counter(dict(nltk.pos_tag(s)).values())\n",
    "\n",
    "        for item, value in count_pos.items():\n",
    "            partial_count[item] = partial_count.get(item, 0) + 1\n",
    "            \n",
    "        total_count.append(partial_count)\n",
    "\n",
    "    return total_count\n",
    "\n",
    "\n",
    "\n",
    "total_count = count_pos_tag(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CC', 'CD', 'DT', 'EX', 'FW', 'JJ', 'JJR', 'JJS', 'MD', 'NN', 'NNP',\n",
       "       'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP',\n",
       "       'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP',\n",
       "       'WP$', 'WRB'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = pd.DataFrame(total_count)\n",
    "pos_df = pos_df.drop(['$', '.', ':', 'IN'], axis = 1)\n",
    "pos_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df['NOUN'] = pos_df[pos_family['NOUN']].sum(axis=1)\n",
    "pos_df['PRON'] = pos_df[pos_family['PRON']].sum(axis=1)\n",
    "pos_df['VERB'] = pos_df[pos_family['VERB']].sum(axis=1)\n",
    "pos_df['ADJ'] = pos_df[pos_family['ADJ']].sum(axis=1)\n",
    "pos_df['ADV'] = pos_df[pos_family['ADV']].sum(axis=1)\n",
    "\n",
    "pos_df = pos_df[['NOUN', 'PRON', 'VERB', 'ADJ', 'ADV']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 29)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data, pos_df], axis = 1)\n",
    "data = data.fillna(value=0.0)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 29 columns):\n",
      "tweet_id                           14640 non-null int64\n",
      "airline_sentiment                  14640 non-null object\n",
      "airline_sentiment_confidence       14640 non-null float64\n",
      "negativereason                     14640 non-null object\n",
      "negativereason_confidence          14640 non-null float64\n",
      "airline                            14640 non-null object\n",
      "airline_sentiment_gold             14640 non-null object\n",
      "name                               14640 non-null object\n",
      "negativereason_gold                14640 non-null object\n",
      "retweet_count                      14640 non-null int64\n",
      "text                               14640 non-null object\n",
      "tweet_coord                        14640 non-null object\n",
      "tweet_created                      14640 non-null object\n",
      "tweet_location                     14640 non-null object\n",
      "user_timezone                      14640 non-null object\n",
      "text_preprocessed                  14640 non-null object\n",
      "neg_scores                         14640 non-null float64\n",
      "neu_scores                         14640 non-null float64\n",
      "pos_scores                         14640 non-null float64\n",
      "compound_scores                    14640 non-null float64\n",
      "text_preprocessed_with_emoticon    14640 non-null object\n",
      "text_stemming_with_emoticon        14640 non-null object\n",
      "text_stemming                      14640 non-null object\n",
      "negation                           14640 non-null bool\n",
      "NOUN                               14640 non-null float64\n",
      "PRON                               14640 non-null float64\n",
      "VERB                               14640 non-null float64\n",
      "ADJ                                14640 non-null float64\n",
      "ADV                                14640 non-null float64\n",
      "dtypes: bool(1), float64(11), int64(2), object(15)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# let's see how the data set is now\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 29)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14076, 29)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As final steps, let's remove duplicates\n",
    "data.drop_duplicates(subset=['text_stemming'], inplace=True)\n",
    "data.drop_duplicates(subset=['text_stemming_with_emoticon'], inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_preprocessed_with_pos.csv', sep ='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
